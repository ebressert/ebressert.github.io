---
layout:     post
title:      Python on steroids
date:       2013-02-25
summary:    While I was looking on stackoverflow for a problem related to D3.js I stumbled on a post about speeding up Python that I couldnâ€™t resist answering. The problem is about convolving two 3D arrays (A and B) that result in a new array (C) with padding on each side. Arrays A and B cubes, so they are symmetrical in dimensions.
---

While I was looking on stackoverflow for a [problem](http://stackoverflow.com/questions/14939325/process-multiple-polygons-d3) related to [D3.js](http://d3js.org/) I stumbled on a post about speeding up Python that I couldn't resist [answering](http://stackoverflow.com/questions/14786920/convolution-of-two-three-dimensional-arrays-with-padding-on-one-side-too-slow/15067190#15067190). The problem is about convolving two 3D arrays (A and B) that result in a new array (C) with padding on each side. Arrays A and B cubes, so they are symmetrical in dimensions.

Here's the code that was posted on stackoverflow.

{% highlight python linenos %}
import numpy as np

dimA = A.shape[0]
dimB = B.shape[0]
dimC = dimA+dimB

C = np.zeros((dimC,dimC,dimC))
for x1 in range(dimA):
    for x2 in range(dimB):
        for y1 in range(dimA):
            for y2 in range(dimB):
                for z1 in range(dimA):
                    for z2 in range(dimB):
                        x = x1+x2
                        y = y1+y2
                        z = z1+z2
                        C[x,y,z] += A[x1,y1,z1] * B[x2,y2,z2]
{% endhighlight %}


The problem is simple, but running this in Python is painful because of the loops. I immediately saw this as a good opportunity to put Numba to the test. With some code modification, I came up with the following to see how well Numba could perform.

{% highlight python linenos %}
import numpy as np
from numba import jit, double

# Creating some fake data to test this problem
s = 15
array_a = np.random.rand(s ** 3).reshape(s, s, s)
array_b = np.random.rand(s ** 3).reshape(s, s, s)

# Original code now in functional form
def custom_convolution(A, B):

    dimA = A.shape[0]
    dimB = B.shape[0]
    dimC = dimA + dimB

    C = np.zeros((dimC, dimC, dimC))
    for x1 in range(dimA):
        for x2 in range(dimB):
            for y1 in range(dimA):
                for y2 in range(dimB):
                    for z1 in range(dimA):
                        for z2 in range(dimB):
                            x = x1 + x2
                            y = y1 + y2
                            z = z1 + z2
                            C[x, y, z] += A[x1, y1, z1]\
                                        * B[x2, y2, z2]
    return C

# Numba'ing the function with the JIT compiler
fast_convolution = jit(double[:, :, :](double[:, :, :],
                        double[:, :, :]))(custom_convolution)
{% endhighlight %}


Okay, so what happened here? I called in two functions from Numba: `jit` and `double`. The `jit` function wraps the original function `custom_convolution` and makes an LLVM version of the code. Then a Python wrapper wraps around that LLVM code. This all happens on the backend, so we don't see the nitty gritty details.

The `double` function is telling JIT compiler that the input values and output values are double precision. You have to declare which dimensions to consider for the input and output as well. Here's a  breakdown of what the input and output parameters details are.

{% highlight python linenos %}
# Calling the jit function with the output parameters
jit(double[:, :, :]\

# The parameters for the two input variables
(double[:, :, :], double[:, :, :]))\

# The non-optimized function
(custom_convolution)
{% endhighlight %}


Using IPython's `%timeit` magic function, we can see how much faster the optimized version of the function is. The results for the original function (non-optimized) is ~18 s, while Numba's optimized function took 10.4 ms. That's a speed up of more than 1700. This blew me away as I was not expecting such a speed up.

In the stackoverflow post, the person who posted the problem mentioned that you could do it with `scipy.signal.fftconvolve`, but some modifications have to be made to make it work right. Out of curiosity I tested the function to see how much *faster* it would be vs the Numba implementation. I was wrong, it was not faster but **slower** by a factor of a few. Impressive.

Take away message? Numba is powerful and allows programmers to boost their code in speed and power with very little changes. Numba works with normal Python objects and NumPy arrays. It does not work on SciPy functions for example. Considering how easy it can be to speed up code with Numba, I have a long and enjoyable list of code to optimize.
